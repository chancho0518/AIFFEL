{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cheap-vegetable",
   "metadata": {},
   "source": [
    "# 4. 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-quick",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드 및 라이브러리 불러오기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-consolidation",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ mkdir -p ~/aiffel/lyricist/models\n",
    "$ ln -s ~/data ~/aiffel/lyricist/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moderate-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-praise",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽어오기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worldwide-lease",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\", \"I can't get no relief Business men, they drink my wine\", 'Plowman dig my earth', 'None were level on the mind', 'Nobody up at his word', 'Hey, hey No reason to get excited', 'The thief he kindly spoke', 'There are many here among us', 'Who feel that life is but a joke', \"But, uh, but you and I, we've been through that\", 'And this is not our fate', \"So let us stop talkin' falsely now\", \"The hour's getting late, hey All along the watchtower\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 폴더의 txt 파일을 모두 읽어서 raw_corpus 에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-procedure",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-creature",
   "metadata": {},
   "source": [
    "#### 먼저 15개의 문장을 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "matched-nancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There must be some kind of way outta here\n",
      "Said the joker to the thief\n",
      "There's too much confusion\n",
      "I can't get no relief Business men, they drink my wine\n",
      "Plowman dig my earth\n",
      "None were level on the mind\n",
      "Nobody up at his word\n",
      "Hey, hey No reason to get excited\n",
      "The thief he kindly spoke\n",
      "There are many here among us\n",
      "Who feel that life is but a joke\n",
      "But, uh, but you and I, we've been through that\n",
      "And this is not our fate\n",
      "So let us stop talkin' falsely now\n",
      "The hour's getting late, hey All along the watchtower\n",
      "Princes kept the view\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너띄기\n",
    "    if idx > 15: break   # 먼저 문장 15개 확인해보기\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-advisory",
   "metadata": {},
   "source": [
    "#### 함수를 통해 입력된 문장을 정제할 것입니다.\n",
    "1. 소문자로 바꾸고, 양쪽 공백 지우기\n",
    "2. 특수문자 양쪽에 공백넣기\n",
    "3. 여러개의 공백은 하나의 공백으로 바꾸기\n",
    "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾸기\n",
    "5. 다시 양쪽의 공백 지우기\n",
    "6. 문장의 시작에는 ```<start>```, 끝에는 ```<end>``` 추가하기\n",
    "\n",
    "#### 그리고 마지막으로 문장이 잘 정제되는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "freelance-brooks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-consciousness",
   "metadata": {},
   "source": [
    "#### 위에서 만든 정제 함수를 이용해서 정제 데이터를 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vietnamese-cyprus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 156227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> there must be some kind of way outta here <end>',\n",
       " '<start> said the joker to the thief <end>',\n",
       " '<start> there s too much confusion <end>',\n",
       " '<start> i can t get no relief business men , they drink my wine <end>',\n",
       " '<start> plowman dig my earth <end>',\n",
       " '<start> none were level on the mind <end>',\n",
       " '<start> nobody up at his word <end>',\n",
       " '<start> hey , hey no reason to get excited <end>',\n",
       " '<start> the thief he kindly spoke <end>',\n",
       " '<start> there are many here among us <end>',\n",
       " '<start> who feel that life is but a joke <end>',\n",
       " '<start> and this is not our fate <end>',\n",
       " '<start> so let us stop talkin falsely now <end>',\n",
       " '<start> the hour s getting late , hey all along the watchtower <end>',\n",
       " '<start> princes kept the view <end>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 공백인 문장은 건너뜁니다\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    \n",
    "    # 정제한 데이터 셋을 저장합니다.\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    # 정제한 데이터가 공백인 경우는 제외합니다.\n",
    "    if len(preprocessed_sentence) == 0:\n",
    "        continue\n",
    "    # 정제된 문장의 구성요소가 15개를 초과하는 경우는 제외하였습니다.\n",
    "    elif len(preprocessed_sentence.split()) > 15:\n",
    "        continue\n",
    "    else:\n",
    "        corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 데이터의 크기와 15개의 데이터를 먼저 확인해보겠습니다.\n",
    "print('데이터 크기:', len(corpus))\n",
    "corpus[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-liquid",
   "metadata": {},
   "source": [
    "#### 입력된 문장을 <u>**토큰화(Tokenize)**</u> 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "employed-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  62 271 ...   0   0   0]\n",
      " [  2 117   6 ...   0   0   0]\n",
      " [  2  62  17 ...   0   0   0]\n",
      " ...\n",
      " [  2  75  45 ...   3   0   0]\n",
      " [  2  49   5 ...   0   0   0]\n",
      " [  2  13 635 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7ff078c2a2d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 13000개의 단어를 기억할 수 있는 tokenizer를 만듭니다.\n",
    "    # 13000개 단어에 포함되지 못한 단어는 '<unk>'로 바꾸겠습니다.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞출 것입니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-danish",
   "metadata": {},
   "source": [
    "#### 생성된 텐서의 3번째 행, 15번째 열까지만 출력해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "joined-federal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   62  271   27   94  546   20   86  743   90    3    0    0    0\n",
      "     0]\n",
      " [   2  117    6 6269   10    6 2310    3    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2   62   17  102  184 2718    3    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : that\n",
      "16 : t\n",
      "17 : s\n",
      "18 : on\n",
      "19 : your\n",
      "20 : of\n",
      "21 : we\n",
      "22 : .\n",
      "23 : like\n",
      "24 : m\n",
      "25 : all\n",
      "26 : is\n",
      "27 : be\n",
      "28 : for\n",
      "29 : up\n",
      "30 : with\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :15])\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 30: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-sauce",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-latvia",
   "metadata": {},
   "source": [
    "#### 훈련 데이터와 평가 데이터를 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bronze-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  62 271  27  94 546  20  86 743  90   3   0   0   0]\n",
      "[ 62 271  27  94 546  20  86 743  90   3   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "single-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_random_state = 35\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
    "    src_input,\n",
    "    tgt_input,\n",
    "    test_size = 0.2,\n",
    "    random_state = n_random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "working-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Target Train: (124981, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "meaningful-enlargement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  230   13   87 9974   76    3    0    0    0    0    0    0]\n",
      "[ 230   13   87 9974   76    3    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(enc_train[1, :-1])\n",
    "print(dec_train[1, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bibliographic-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 생성합니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fantastic-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 1024\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "growing-cooler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-3.60191450e-04,  2.23801297e-04,  1.14300819e-04, ...,\n",
       "          4.95599350e-04,  1.21036814e-04,  5.46641066e-04],\n",
       "        [-1.56331836e-04,  1.73110791e-04,  2.12264829e-04, ...,\n",
       "          1.01504766e-03,  1.76648518e-05,  5.91539312e-04],\n",
       "        [ 1.62272729e-04, -4.29857500e-05,  1.32690591e-03, ...,\n",
       "          1.41716108e-03,  3.53180279e-04,  6.38290658e-04],\n",
       "        ...,\n",
       "        [ 8.24119488e-04, -3.78367887e-03, -6.95905765e-04, ...,\n",
       "         -4.61831875e-03,  1.76911347e-03,  1.76695792e-03],\n",
       "        [ 1.11357006e-03, -4.21933085e-03, -8.43754620e-04, ...,\n",
       "         -5.28054638e-03,  1.94843661e-03,  1.81809650e-03],\n",
       "        [ 1.40855229e-03, -4.63318033e-03, -9.46886488e-04, ...,\n",
       "         -5.85185550e-03,  2.13021622e-03,  1.86953309e-03]],\n",
       "\n",
       "       [[-3.60191450e-04,  2.23801297e-04,  1.14300819e-04, ...,\n",
       "          4.95599350e-04,  1.21036814e-04,  5.46641066e-04],\n",
       "        [-7.65532430e-04,  2.27603436e-04, -4.11028217e-04, ...,\n",
       "          2.93573365e-04,  9.78558528e-05,  1.09006651e-03],\n",
       "        [-8.67620169e-04,  1.70580970e-04, -4.16857889e-04, ...,\n",
       "          2.17697423e-04, -2.46833748e-04,  1.74862565e-03],\n",
       "        ...,\n",
       "        [-9.37102188e-04, -2.22809380e-03, -1.68990577e-03, ...,\n",
       "         -6.07477559e-04,  2.56308733e-04,  1.18315802e-03],\n",
       "        [-1.02524390e-03, -2.62835110e-03, -1.93669565e-03, ...,\n",
       "         -1.42042839e-03,  4.64715675e-04,  1.30724232e-03],\n",
       "        [-1.00963109e-03, -2.98426277e-03, -2.13745399e-03, ...,\n",
       "         -2.30348599e-03,  6.52360555e-04,  1.44571904e-03]],\n",
       "\n",
       "       [[-3.60191450e-04,  2.23801297e-04,  1.14300819e-04, ...,\n",
       "          4.95599350e-04,  1.21036814e-04,  5.46641066e-04],\n",
       "        [-1.49405416e-04,  3.56491182e-05,  7.20765529e-05, ...,\n",
       "          1.15126860e-03, -3.56689794e-04,  9.73812013e-04],\n",
       "        [-1.11525769e-04, -8.05731906e-05,  8.46241237e-05, ...,\n",
       "          1.67815539e-03, -6.86922867e-04,  1.70491950e-03],\n",
       "        ...,\n",
       "        [-6.11089112e-04, -8.27619049e-04, -6.76674012e-04, ...,\n",
       "         -1.67778064e-03,  1.48835941e-04,  1.60027493e-03],\n",
       "        [-4.89962345e-04, -1.39314076e-03, -8.71886034e-04, ...,\n",
       "         -2.62289308e-03,  3.78177472e-04,  1.64915458e-03],\n",
       "        [-2.89892516e-04, -1.98045303e-03, -1.01714768e-03, ...,\n",
       "         -3.48800584e-03,  6.37870864e-04,  1.70069851e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.60191450e-04,  2.23801297e-04,  1.14300819e-04, ...,\n",
       "          4.95599350e-04,  1.21036814e-04,  5.46641066e-04],\n",
       "        [-1.98941911e-04,  2.81432207e-04,  4.08537890e-05, ...,\n",
       "          1.32791605e-03,  3.81533522e-04,  8.06911150e-04],\n",
       "        [-2.77101295e-04,  1.84880613e-04,  2.54868210e-04, ...,\n",
       "          1.30279735e-03,  1.88109203e-04,  1.26533746e-03],\n",
       "        ...,\n",
       "        [-3.18928622e-04,  1.94875451e-04, -2.38802706e-04, ...,\n",
       "         -5.27252385e-04,  4.17565083e-04,  1.64461567e-03],\n",
       "        [-2.15449298e-04, -3.81755584e-04, -5.34863910e-04, ...,\n",
       "         -1.45195925e-03,  5.56974497e-04,  1.79921230e-03],\n",
       "        [-4.70387495e-05, -9.84440208e-04, -7.75480352e-04, ...,\n",
       "         -2.37620785e-03,  7.17382354e-04,  1.90720637e-03]],\n",
       "\n",
       "       [[-3.60191450e-04,  2.23801297e-04,  1.14300819e-04, ...,\n",
       "          4.95599350e-04,  1.21036814e-04,  5.46641066e-04],\n",
       "        [-5.37442626e-04,  1.94830820e-04,  2.31844781e-04, ...,\n",
       "          6.44955726e-04,  3.81104212e-04,  7.26185157e-04],\n",
       "        [ 2.22434683e-05,  1.95502871e-04,  5.90831391e-04, ...,\n",
       "          4.89489175e-04,  3.22001986e-04,  6.32438692e-04],\n",
       "        ...,\n",
       "        [-6.26583525e-04, -2.34728958e-03, -1.43169239e-03, ...,\n",
       "         -2.40008626e-03, -7.07479427e-04,  8.35808576e-04],\n",
       "        [-6.14049728e-04, -2.81316484e-03, -1.59003690e-03, ...,\n",
       "         -3.01516918e-03, -3.08646326e-04,  1.01278315e-03],\n",
       "        [-4.88386489e-04, -3.25765577e-03, -1.68561470e-03, ...,\n",
       "         -3.63317970e-03,  9.56426156e-05,  1.17295759e-03]],\n",
       "\n",
       "       [[-3.60191450e-04,  2.23801297e-04,  1.14300819e-04, ...,\n",
       "          4.95599350e-04,  1.21036814e-04,  5.46641066e-04],\n",
       "        [-5.37442626e-04,  1.94830820e-04,  2.31844781e-04, ...,\n",
       "          6.44955726e-04,  3.81104212e-04,  7.26185157e-04],\n",
       "        [-5.28439065e-04,  8.73361496e-05,  1.64828758e-04, ...,\n",
       "          4.77913971e-04,  6.66485226e-04,  3.45845154e-04],\n",
       "        ...,\n",
       "        [ 9.61185942e-05,  1.92503817e-03,  1.10484217e-03, ...,\n",
       "          9.73449030e-04, -7.97625340e-04,  1.35582429e-03],\n",
       "        [-1.13301438e-04,  1.45765569e-03,  7.26561469e-04, ...,\n",
       "          2.99310486e-04, -8.53558420e-04,  1.49837078e-03],\n",
       "        [-2.31467959e-04,  8.98585247e-04,  3.07176961e-04, ...,\n",
       "         -5.30447462e-04, -8.46409297e-04,  1.62543915e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러보겠습니다.\n",
    "for enc_sample, dec_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어보겠습니다.\n",
    "model(enc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "exterior-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      multiple                  12289024  \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 41,375,457\n",
      "Trainable params: 41,375,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "optional-provider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "488/488 [==============================] - 185s 373ms/step - loss: 3.8962\n",
      "Epoch 2/10\n",
      "488/488 [==============================] - 182s 373ms/step - loss: 2.9722\n",
      "Epoch 3/10\n",
      "488/488 [==============================] - 182s 373ms/step - loss: 2.7653\n",
      "Epoch 4/10\n",
      "488/488 [==============================] - 182s 372ms/step - loss: 2.6151\n",
      "Epoch 5/10\n",
      "488/488 [==============================] - 182s 373ms/step - loss: 2.4796\n",
      "Epoch 6/10\n",
      "488/488 [==============================] - 182s 373ms/step - loss: 2.3610\n",
      "Epoch 7/10\n",
      "488/488 [==============================] - 182s 372ms/step - loss: 2.2444\n",
      "Epoch 8/10\n",
      "488/488 [==============================] - 181s 371ms/step - loss: 2.1388\n",
      "Epoch 9/10\n",
      "488/488 [==============================] - 182s 373ms/step - loss: 2.0347\n",
      "Epoch 10/10\n",
      "488/488 [==============================] - 181s 371ms/step - loss: 1.9312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff00c228c10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train_epochs = 10\n",
    "\n",
    "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = n_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "conditional-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 입력받은 init_sentence을 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "peripheral-scratch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> is not the only one that can be <end> '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> is not\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-chick",
   "metadata": {},
   "source": [
    "## 회고\n",
    "### 1. embedding size와 hidden size의 조정에 따른 성능 변화를 좀더 관찰해보고 싶습니다.\n",
    "### 2. 최적의 파라미터를 찾을 수 있는 방법이 있는지 궁금합니다.\n",
    "### 3. 영어가 아닌 한글 작사의 경우는 어떻게 할 수 있을지 궁금합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
